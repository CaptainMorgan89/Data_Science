import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import os
from tabulate import tabulate
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
train_data = pd.read_csv("https://storage.googleapis.com/courses_data/Machine%20Learning%20Titanic/train.csv")
test_data = pd.read_csv("https://storage.googleapis.com/courses_data/Machine%20Learning%20Titanic/test.csv")

# 2. Drop ÏƒÏ„Î®Î»ÎµÏ‚ Ï€Î¿Ï… Î´ÎµÎ½ Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ (Ticket, Cabin, PassengerId)
train_data = train_data.drop(columns=["PassengerId", "Ticket", "Cabin"])
test_data = test_data.drop(columns=["PassengerId", "Ticket", "Cabin"])

# 3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Title Î±Ï€ÏŒ Name
train_data["Title"] = [x.split(".")[0].split(",")[1].strip() for x in train_data["Name"]]
test_data["Title"] = [x.split(".")[0].split(",")[1].strip() for x in test_data["Name"]]

# 4. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± flag Î³Î¹Î± ÎµÎ¹Î´Î¹ÎºÎ¿ÏÏ‚ Ï„Î¯Ï„Î»Î¿Ï…Ï‚
def is_special_title(title):
    if title in ['Mr', 'Mrs', 'Miss']:
        return 0
    else:
        return 1

train_data['Special_Title'] = train_data['Title'].apply(is_special_title)
test_data['Special_Title'] = test_data['Title'].apply(is_special_title)

# 5. Drop Name ÎºÎ±Î¹ Title
train_data = train_data.drop(columns=["Name", "Title"])
test_data = test_data.drop(columns=["Name", "Title"])

# 6. Map Sex ÏƒÎµ Î±ÏÎ¹Î¸Î¼ÏŒ
train_data["Sex"] = train_data["Sex"].map({"male":0, "female":1})
test_data["Sex"] = test_data["Sex"].map({"male":0, "female":1})

# 7. Fillna ÏƒÏ„Î·Î½ Age
train_data["Age"] = train_data["Age"].fillna(train_data["Age"].mean())
test_data["Age"] = test_data["Age"].fillna(train_data["Age"].mean())  # Ï‡ÏÎ®ÏƒÎ· Î¼Î­ÏƒÎ¿Ï… ÏŒÏÎ¿Ï… Ï„Î¿Ï… train set

# 8. Fillna ÏƒÏ„Î· Fare Ï„Î¿Ï… test set
test_data["Fare"] = test_data["Fare"].fillna(test_data["Fare"].mean())

# 9. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± dummy Î³Î¹Î± Embarked
train_data = pd.concat([train_data, pd.get_dummies(train_data["Embarked"], prefix="Embarked")], axis=1)
test_data = pd.concat([test_data, pd.get_dummies(test_data["Embarked"], prefix="Embarked")], axis=1)

train_data = train_data.drop(columns=["Embarked"])
test_data = test_data.drop(columns=["Embarked"])

# 10. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± dummy Î³Î¹Î± Pclass
train_data = pd.concat([train_data, pd.get_dummies(train_data["Pclass"], prefix="Pclass")], axis=1)
test_data = pd.concat([test_data, pd.get_dummies(test_data["Pclass"], prefix="Pclass")], axis=1)

train_data = train_data.drop(columns=["Pclass"])
test_data = test_data.drop(columns=["Pclass"])

# 11. Î§Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ X ÎºÎ±Î¹ y (train)
y_train = train_data["Survived"]
X_train = train_data.drop(columns=["Survived"])

# 12. Scaling Î¼Îµ MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(test_data)

X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=test_data.columns)

train_scaled = pd.concat([X_train_scaled_df, y_train.reset_index(drop=True)], axis=1)

# 13. Î•ÎºÏ„ÏÏ€Ï‰ÏƒÎ· Ï€ÏÏÏ„Ï‰Î½ 5 Î³ÏÎ±Î¼Î¼ÏÎ½ ÏƒÏ„Î·Î½ ÎºÎ¿Î½ÏƒÏŒÎ»Î±
print("\n--- Train Data (scaled) ---")
print(tabulate(train_scaled.head(), headers='keys', tablefmt='psql'))

print("\n--- Test Data (scaled) ---")
print(tabulate(X_test_scaled_df.head(), headers='keys', tablefmt='psql'))

# 14. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
model = LogisticRegression(max_iter=800)
model.fit(X_train_scaled, y_train)

# 15. Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î³Î¹Î± test set
test_predictions = model.predict(X_test_scaled)

# 16. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± DataFrame Î³Î¹Î± Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·
submission = pd.DataFrame({
    "PassengerId": pd.read_csv("https://storage.googleapis.com/courses_data/Machine%20Learning%20Titanic/test.csv")["PassengerId"],
    "Survived": test_predictions
})

# 17. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î·Î½ Î•Ï€Î¹Ï†Î¬Î½ÎµÎ¹Î± Î•ÏÎ³Î±ÏƒÎ¯Î±Ï‚
desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
file_path = os.path.join(desktop_path, "submission.csv")


submission.to_csv(file_path, index=False, sep=',')

print(f"\nÎ¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ 'submission.csv' Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ ÏƒÏ„Î·Î½ Î•Ï€Î¹Ï†Î¬Î½ÎµÎ¹Î± Î•ÏÎ³Î±ÏƒÎ¯Î±Ï‚: {file_path}")

# 18. Î ÏÎ¿Î²Î¿Î»Î® Ï„Ï‰Î½ Ï€ÏÏÏ„Ï‰Î½ 10 Î³ÏÎ±Î¼Î¼ÏÎ½ Ï„Î¿Ï… submission
print("\n--- Submission preview (first 10 rows) ---")
print(tabulate(submission.head(10), headers='keys', tablefmt='psql'))


# Î›Î¯ÏƒÏ„Î± Î¼Îµ Î¼Î¿Î½Ï„Î­Î»Î± Î³Î¹Î± Î´Î¿ÎºÎ¹Î¼Î®
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Support Vector Machine": SVC(),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5)
}

# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÎºÎ±Î¹ Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
for name, clf in models.items():
    clf.fit(X_train_scaled, y_train)
    y_pred = clf.predict(X_train_scaled)
    acc = accuracy_score(y_train, y_pred)
    print(f"\nğŸ”¹ Model: {name}")
    print(f"Training Accuracy: {acc:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_train, y_pred))
    print("Classification Report:")
    print(classification_report(y_train, y_pred))

    # Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÏƒÏ„Î¿ test set
    test_pred = clf.predict(X_test_scaled)

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± submission file Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿
    submission = pd.DataFrame({
        "PassengerId": pd.read_csv("https://storage.googleapis.com/courses_data/Machine%20Learning%20Titanic/test.csv")["PassengerId"],
        "Survived": test_pred
    })

    # Save Î¼Îµ ÏŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï… Î±Î½Î¬Î»Î¿Î³Î± Î¼Îµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
    filename = f"submission_{name.replace(' ', '_')}.csv"
    submission.to_csv(os.path.join(desktop_path, filename), index=False)
    print(f"âœ… Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {filename}")
