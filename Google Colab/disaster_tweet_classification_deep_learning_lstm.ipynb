{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1GAfXjz-YRqWxbpAoz03bPD08sfNCetD3","authorship_tag":"ABX9TyPZMkP+luOUjFYldKDD22w0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"id":"fEzumMSN3oga","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1754741892245,"user_tz":-180,"elapsed":105655,"user":{"displayName":"Captain89","userId":"09609436795190012982"}},"outputId":"f188c0d8-bbc5-43e4-82c0-836fb3aad780"},"outputs":[{"output_type":"stream","name":"stdout","text":["First few rows of the DataFrame:\n","   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  \n","\n","DataFrame shape:\n","(7613, 5)\n","\n","Number of entries labeled as 'Disaster':\n","3271\n","\n","Number of entries labeled as 'No Disaster':\n","4342\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Total number of unique words: 15950\n","\n","Most Common words: [('like', 346), ('amp', 344), ('fire', 249), ('get', 228), ('new', 223)]\n","\n","Train and Test sets are:\n","(6090,) (1523,)\n","\n","Sample of word index dictionary (first 10):\n","{'like': 1, 'amp': 2, 'fire': 3, 'get': 4, 'via': 5, 'new': 6, 'people': 7, 'news': 8, 'one': 9, 'video': 10}\n","\n","Shape of padded training sequences: (6090, 20)\n","Shape of padded validation sequences: (1523, 20)\n","\n","Example original sentence:\n","one direction pick fan army directioners x1402\n","\n","Sequence representation:\n","[9, 992, 653, 536, 101, 1606, 5375]\n","\n","Padded sequence:\n","[   9  992  653  536  101 1606 5375    0    0    0    0    0    0    0\n","    0    0    0    0    0    0]\n","\n","Decoded back:\n","one direction pick fan army directioners x1402\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │       \u001b[38;5;34m510,400\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,832\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">510,400</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m535,297\u001b[0m (2.04 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,297</span> (2.04 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m535,297\u001b[0m (2.04 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,297</span> (2.04 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","191/191 - 7s - 37ms/step - accuracy: 0.7061 - loss: 0.5552 - val_accuracy: 0.7912 - val_loss: 0.4618\n","Epoch 2/20\n","191/191 - 5s - 24ms/step - accuracy: 0.8783 - loss: 0.3075 - val_accuracy: 0.7965 - val_loss: 0.4879\n","Epoch 3/20\n","191/191 - 4s - 22ms/step - accuracy: 0.9386 - loss: 0.1784 - val_accuracy: 0.7859 - val_loss: 0.5193\n","Epoch 4/20\n","191/191 - 4s - 22ms/step - accuracy: 0.9619 - loss: 0.1270 - val_accuracy: 0.7768 - val_loss: 0.6821\n","Epoch 5/20\n","191/191 - 5s - 25ms/step - accuracy: 0.9691 - loss: 0.0992 - val_accuracy: 0.7768 - val_loss: 0.7209\n","Epoch 6/20\n","191/191 - 7s - 36ms/step - accuracy: 0.9752 - loss: 0.0843 - val_accuracy: 0.7748 - val_loss: 0.6546\n","Epoch 7/20\n","191/191 - 5s - 27ms/step - accuracy: 0.9767 - loss: 0.0702 - val_accuracy: 0.7669 - val_loss: 0.7837\n","Epoch 8/20\n","191/191 - 3s - 18ms/step - accuracy: 0.9782 - loss: 0.0572 - val_accuracy: 0.7689 - val_loss: 1.1326\n","Epoch 9/20\n","191/191 - 4s - 22ms/step - accuracy: 0.9806 - loss: 0.0475 - val_accuracy: 0.7630 - val_loss: 1.1195\n","Epoch 10/20\n","191/191 - 4s - 22ms/step - accuracy: 0.9793 - loss: 0.0510 - val_accuracy: 0.7617 - val_loss: 0.8849\n","Epoch 11/20\n","191/191 - 3s - 16ms/step - accuracy: 0.9824 - loss: 0.0405 - val_accuracy: 0.7459 - val_loss: 1.0849\n","Epoch 12/20\n","191/191 - 6s - 29ms/step - accuracy: 0.9811 - loss: 0.0409 - val_accuracy: 0.7564 - val_loss: 1.2180\n","Epoch 13/20\n","191/191 - 5s - 27ms/step - accuracy: 0.9828 - loss: 0.0354 - val_accuracy: 0.7492 - val_loss: 1.3624\n","Epoch 14/20\n","191/191 - 5s - 24ms/step - accuracy: 0.9821 - loss: 0.0371 - val_accuracy: 0.7446 - val_loss: 1.4975\n","Epoch 15/20\n","191/191 - 3s - 17ms/step - accuracy: 0.9796 - loss: 0.0481 - val_accuracy: 0.7387 - val_loss: 1.1975\n","Epoch 16/20\n","191/191 - 5s - 26ms/step - accuracy: 0.9839 - loss: 0.0361 - val_accuracy: 0.7525 - val_loss: 1.2568\n","Epoch 17/20\n","191/191 - 3s - 17ms/step - accuracy: 0.9841 - loss: 0.0315 - val_accuracy: 0.7531 - val_loss: 1.2464\n","Epoch 18/20\n","191/191 - 5s - 26ms/step - accuracy: 0.9842 - loss: 0.0304 - val_accuracy: 0.7603 - val_loss: 1.6342\n","Epoch 19/20\n","191/191 - 6s - 34ms/step - accuracy: 0.9836 - loss: 0.0316 - val_accuracy: 0.7360 - val_loss: 1.5949\n","Epoch 20/20\n","191/191 - 4s - 20ms/step - accuracy: 0.9816 - loss: 0.0439 - val_accuracy: 0.7590 - val_loss: 1.1960\n","\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import layers, models\n","\n","# Read the CSV file into a DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/train.csv')\n","\n","# Print the first few rows of the DataFrame (head)\n","print(\"First few rows of the DataFrame:\")\n","print(df.head())\n","\n","# Print the shape of the DataFrame\n","print(\"\\nDataFrame shape:\")\n","print(df.shape)\n","\n","# Print the number of entries labeled as 'Disaster' (target == 1)\n","print(\"\\nNumber of entries labeled as 'Disaster':\")\n","print((df.target == 1).sum())\n","\n","# Print the number of entries labeled as 'No Disaster' (target == 0)\n","print(\"\\nNumber of entries labeled as 'No Disaster':\")\n","print((df.target == 0).sum())\n","\n","# Download NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Convert text in 'text' column to lowercase\n","df['text'] = df['text'].apply(lambda x: x.lower())\n","\n","# Define preprocessing functions\n","def remove_URL(text):\n","    url_pattern = r'https?://\\S+|www\\.\\S+'\n","    return re.sub(url_pattern, '', text)\n","\n","def remove_punct(text):\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.isalnum()]\n","    return ' '.join(filtered_words)\n","\n","def remove_stopwords(text):\n","    words = text.split()\n","    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n","    return ' '.join(filtered_words)\n","\n","# Apply preprocessing\n","df[\"text\"] = df.text.map(remove_URL)\n","df[\"text\"] = df.text.map(remove_punct)\n","df[\"text\"] = df.text.map(remove_stopwords)\n","\n","# Function to count unique words\n","def counter_word(text_col):\n","    count = Counter()\n","    for text in text_col.values:\n","        for word in text.split():\n","            count[word] += 1\n","    return count\n","\n","counter = counter_word(df['text'])\n","\n","print(\"\")\n","print(\"Total number of unique words:\", len(counter))\n","print(\"\\nMost Common words:\", counter.most_common(5))\n","\n","# Define features and labels\n","X = df['text'].values\n","y = df['target'].values\n","\n","# Split data\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n","    X, y, test_size=0.2, random_state=42)\n","print(\"\\nTrain and Test sets are:\")\n","print(train_sentences.shape, val_sentences.shape)\n","\n","# Tokenizer setup\n","num_unique_words = len(counter)\n","tokenizer = Tokenizer(num_words=num_unique_words)\n","tokenizer.fit_on_texts(train_sentences)\n","\n","word_index = tokenizer.word_index\n","print(\"\\nSample of word index dictionary (first 10):\")\n","print(dict(list(word_index.items())[:10]))\n","\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","val_sequences = tokenizer.texts_to_sequences(val_sentences)\n","\n","# Padding\n","max_length = 20\n","train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n","val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n","\n","print(\"\\nShape of padded training sequences:\", train_padded.shape)\n","print(\"Shape of padded validation sequences:\", val_padded.shape)\n","\n","# Reverse word index for decoding\n","reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n","\n","def decode(sequence):\n","    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n","\n","print(\"\\nExample original sentence:\")\n","print(train_sentences[10])\n","print(\"\\nSequence representation:\")\n","print(train_sequences[10])\n","print(\"\\nPadded sequence:\")\n","print(train_padded[10])\n","print(\"\\nDecoded back:\")\n","print(decode(train_sequences[10]))\n","\n","# Build the model\n","model = models.Sequential()\n","model.add(layers.Embedding(input_dim=num_unique_words, output_dim=32, input_shape=(max_length,)))\n","model.add(layers.LSTM(64, dropout=0.1))\n","model.add(layers.Dense(1, activation=\"sigmoid\"))\n","\n","model.summary()\n","\n","# Compile the model\n","loss = keras.losses.BinaryCrossentropy(from_logits=False)\n","optim = keras.optimizers.Adam(learning_rate=0.001)\n","metrics = [\"accuracy\"]\n","model.compile(loss=loss, optimizer=optim, metrics=metrics)\n","\n","# Train the model\n","model.fit(\n","    train_padded,\n","    train_labels,\n","    epochs=20,\n","    validation_data=(val_padded, val_labels),\n","    verbose=2\n",")\n","\n","\n","# Make predictions using the trained model on the padded training sequences\n","predictions = model.predict(train_padded)\n","\n","# Convert predicted probabilities to binary labels using a threshold of 0.5\n","predictions = [1 if p > 0.5 else 0 for p in predictions]"]}]}