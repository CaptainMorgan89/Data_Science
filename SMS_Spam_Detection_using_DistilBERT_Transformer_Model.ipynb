{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BKAdsboDJzH7TPUEOjF42n1yAuwSD5So",
      "authorship_tag": "ABX9TyNXgddT1EudLgxokELfACvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaptainMorgan89/Data_Science/blob/master/SMS_Spam_Detection_using_DistilBERT_Transformer_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7FR9-YbeZ9",
        "outputId": "c014842e-e725-435e-dbb3-912db465831a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5572, 2)\n",
            "   label                                            message\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "message: 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
            "input ids: [101, 2175, 2127, 18414, 17583, 2391, 1010, 4689, 1012, 1012, 2800, 2069, 1999, 11829, 2483, 1050, 2307, 2088, 2474, 102]\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 592s 4s/step - loss: 0.4236 - accuracy: 0.8519\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 562s 4s/step - loss: 0.4002 - accuracy: 0.8685\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 570s 4s/step - loss: 0.3983 - accuracy: 0.8663\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 568s 4s/step - loss: 0.3886 - accuracy: 0.8732\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 568s 4s/step - loss: 0.3989 - accuracy: 0.8665\n",
            "35/35 [==============================] - 53s 1s/step - loss: 0.4408 - accuracy: 0.8637\n",
            "{'loss': 0.44076311588287354, 'accuracy': 0.8636771440505981}\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "0.9427204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at ./model/clf were not used when initializing TFDistilBertForSequenceClassification: ['dropout_199']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/clf and are newly initialized: ['dropout_219']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "0.9427204\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations, losses\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Reading the SMS spam collection dataset from a CSV file into a Pandas DataFrame\n",
        "# The 'sep' parameter specifies that the data is separated by tabs ('\\t')\n",
        "# The 'names' parameter assigns names to the columns of the DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
        "# Printing the shape of the DataFrame\n",
        "print(df.shape)\n",
        "# Mapping the labels 'ham' and 'spam' to numeric values 0 and 1, respectively\n",
        "# This is done to convert the labels into a format suitable for machine learning models\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "# Displaying the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())\n",
        "# Extracting the 'message' column from the DataFrame and converting it to a list\n",
        "x = list(df['message'])\n",
        "# Extracting the 'label' column from the DataFrame and converting it to a list\n",
        "y = list(df['label'])\n",
        "\n",
        "\n",
        "# Define the model name and maximum length of tokens\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "MAX_LEN = 20\n",
        "# Select the first message from the dataset\n",
        "message = x[0]\n",
        "# Load the tokenizer for the specified DistilBERT model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "# Tokenize the message using the tokenizer, ensuring it has a maximum length of MAX_LEN\n",
        "inputs = tokenizer(message, max_length=MAX_LEN, truncation=True, padding=True)\n",
        "# Print the original message, tokenized input IDs, and attention mask\n",
        "print(f'message: \\'{message}\\'')\n",
        "print(f'input ids: {inputs[\"input_ids\"]}')\n",
        "print(f'attention mask: {inputs[\"attention_mask\"]}')\n",
        "\n",
        "\n",
        "def construct_encodings(x, tkzr, max_len, truncation=True, padding=True):\n",
        "    \"\"\"\n",
        "    Function to construct encodings for input sequences using a tokenizer.\n",
        "\n",
        "    Args:\n",
        "    - x: Input sequences to be encoded.\n",
        "    - tkzr: Tokenizer object used for encoding.\n",
        "    - max_len: Maximum length of the encoded sequences.\n",
        "    - trucation: Whether to truncate sequences to `max_len`.\n",
        "    - padding: Whether to pad sequences to `max_len`.\n",
        "\n",
        "    Returns:\n",
        "    - Encodings: Encoded representations of the input sequences.\n",
        "\n",
        "    \"\"\"\n",
        "    # Use the tokenizer to encode the input sequences\n",
        "    encodings = tkzr(\n",
        "        x,\n",
        "        max_length=max_len,  # Set the maximum length of the encoded sequences\n",
        "        truncation=truncation,  # Truncate sequences if they exceed `max_len`\n",
        "        padding=padding  # Pad sequences if they are shorter than `max_len`\n",
        "    )\n",
        "    return encodings\n",
        "# Call the construct_encodings function to encode input sequences\n",
        "encodings = construct_encodings(x, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "\n",
        "# Define a function to construct a TensorFlow Dataset from token encodings and labels (if provided)\n",
        "def construct_tfdataset(encodings, y=None):\n",
        "    # If labels are provided (during training or evaluation)\n",
        "    if y:\n",
        "        # Create a TensorFlow Dataset from token encodings and labels\n",
        "        # by slicing the encodings dictionary and combining it with the labels\n",
        "        return tf.data.Dataset.from_tensor_slices((dict(encodings), y))\n",
        "    else:\n",
        "        # If labels are not provided (during inference/prediction)\n",
        "        # This case is used when making predictions on unseen samples after training\n",
        "        # Create a TensorFlow Dataset only from token encodings\n",
        "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
        "# Call the construct_tfdataset function with token encodings and labels (if available)\n",
        "tfdataset = construct_tfdataset(encodings, y)\n",
        "\n",
        "\n",
        "# Define the ratio of the dataset to be used for testing\n",
        "TEST_SPLIT = 0.2\n",
        "# Define the batch size for training and testing data\n",
        "BATCH_SIZE = 32\n",
        "# Calculate the size of the training set based on the test split ratio\n",
        "train_size = int(len(x) * (1 - TEST_SPLIT))\n",
        "# Shuffle the dataset to ensure randomness in training and testing samples\n",
        "tfdataset = tfdataset.shuffle(len(x))\n",
        "# Split the dataset into training and testing sets based on the calculated size\n",
        "tfdataset_train = tfdataset.take(train_size)  # Take the first `train_size` samples for training\n",
        "tfdataset_test = tfdataset.skip(train_size)    # Skip the first `train_size` samples for testing\n",
        "# Batch the training and testing datasets using the specified batch size\n",
        "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)  # Batch the training set\n",
        "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)    # Batch the testing set\n",
        "\n",
        "\n",
        "# Define the number of epochs for training\n",
        "N_EPOCHS = 5\n",
        "# Load the pre-trained DistilBERT model for sequence classification\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, from_pt=True)\n",
        "# Define the loss function for the model\n",
        "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Compile the model with the specified loss function and evaluation metric\n",
        "model.compile(loss=loss, metrics=['accuracy'])\n",
        "# Train the model on the training dataset\n",
        "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)\n",
        "\n",
        "# Evaluate the trained model on the test dataset\n",
        "# The model.evaluate() function computes performance metrics such as loss and accuracy\n",
        "# It takes the test dataset (tfdataset_test) as input and returns evaluation results\n",
        "# We set return_dict=True to return evaluation results as a dictionary\n",
        "# The batch_size parameter specifies the number of samples processed per\n",
        "# batch during evaluation\n",
        "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(benchmarks)\n",
        "\n",
        "# Define a function to create a predictor for text classification\n",
        "def create_predictor(model, model_name, max_len):\n",
        "    # Load the tokenizer for the specified pre-trained DistilBERT model\n",
        "    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Define a nested function to predict probabilities for text input\n",
        "    def predict_proba(text):\n",
        "        # Preprocess the text input\n",
        "        x = [text]\n",
        "\n",
        "        # Construct input encodings using the tokenizer\n",
        "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
        "\n",
        "        # Construct a TensorFlow dataset from the input encodings\n",
        "        tfdataset = construct_tfdataset(encodings)\n",
        "\n",
        "        # Batch the dataset with a batch size of 1\n",
        "        tfdataset = tfdataset.batch(1)\n",
        "\n",
        "        # Use the provided model to predict logits for the text input\n",
        "        preds = model.predict(tfdataset).logits\n",
        "\n",
        "        # Apply softmax activation to convert logits to probabilities\n",
        "        preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
        "\n",
        "        # Return the predicted probability of the positive class\n",
        "        return preds[0][0]\n",
        "\n",
        "    # Return the nested prediction function\n",
        "    return predict_proba\n",
        "\n",
        "# Create a predictor function using the provided model, model name,\n",
        "# and maximum sequence length\n",
        "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
        "\n",
        "# Test the predictor function with a sample text input and print the predicted probability\n",
        "print(clf('New Job opportunity for you in Dubai'))\n",
        "\n",
        "\n",
        "# Saving the trained model to the specified directory\n",
        "model.save_pretrained('./model/clf')\n",
        "\n",
        "# Saving metadata (such as model name and maximum length) using pickle\n",
        "# Here, MODEL_NAME and MAX_LEN are assumed to be variables containing relevant information\n",
        "# 'wb' mode is used to write binary data to the file\n",
        "with open('./model/info.pkl', 'wb') as f:\n",
        "    # Dumping the tuple containing metadata into the pickle file\n",
        "    pickle.dump((MODEL_NAME, MAX_LEN), f)\n",
        "\n",
        "\n",
        "    # Load the pre-trained DistilBERT model from the specified directory\n",
        "new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf')\n",
        "# Load additional information such as the model name and maximum sequence\n",
        "# length from a pickled file\n",
        "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))\n",
        "\n",
        "\n",
        "# Create a predictor function using the loaded model, model name,\n",
        "# and maximum sequence length\n",
        "clf = create_predictor(new_model, model_name, max_len)\n",
        "\n",
        "# Test the predictor function with a sample text input\n",
        "print(clf('NEw Job opportunity for you in Dubai'))"
      ]
    }
  ]
}